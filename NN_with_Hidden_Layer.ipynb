{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self,D_in,H_in,D_out): #The __init__ function initialises the two linear layers of the model.\n",
    "        super(NN,self).__init__()\n",
    "        self.Linear1 = nn.Linear(D_in,H_in)\n",
    "        self.Linear2 = nn.Linear(H_in,D_out)\n",
    "        \n",
    "    def forward(self,x): #In the forward function, we first apply the first linear layer, apply ReLU activation and then apply the second linear layer.\n",
    "        in_lin = self.Linear1(x) # Here the x is assumed to be the batch size (no.of training examples) by the model\n",
    "        # If the network takes in the input vector of dim 100 and a batch size of 50 is passed in the model above, then the dim of x would be (50 x 100).\n",
    "        h_relu = F.relu(in_lin)\n",
    "        out_lin = self.Linear2(h_relu)\n",
    "        y_pred = F.relu(out_lin)\n",
    "        return y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "D_in,H_in,D_out,batch_size = 100,50,25,50\n",
    "# Create nodes in pytorch using Variable present in torch.autograd package\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n",
    "x = torch.autograd.Variable(torch.randn(batch_size,D_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(D_in,H_in,D_out)\n",
    "y_pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randn(batch_size,D_out).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(y_pred,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9971992373466492\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0861, -0.0635,  0.0343,  ...,  0.0829, -0.0528, -0.0113],\n",
      "        [ 0.0040, -0.0725,  0.0304,  ...,  0.0191, -0.0957,  0.0291],\n",
      "        [-0.0359,  0.0433,  0.0952,  ..., -0.0227, -0.0311, -0.0826],\n",
      "        ...,\n",
      "        [ 0.0220,  0.0456, -0.0435,  ...,  0.0858, -0.0920, -0.0151],\n",
      "        [ 0.0739,  0.0794,  0.0547,  ..., -0.0221, -0.0510, -0.0922],\n",
      "        [ 0.0499,  0.0184, -0.0918,  ...,  0.0203,  0.0734, -0.0595]],\n",
      "       dtype=torch.float32, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0485, -0.0937, -0.0077, -0.0073, -0.0168,  0.0526,  0.0825, -0.0883,\n",
      "         0.0519,  0.0624,  0.0429,  0.0649, -0.0329, -0.0161,  0.0643,  0.0492,\n",
      "         0.0950, -0.0428, -0.0046,  0.0239, -0.0096,  0.0077,  0.0953, -0.0314,\n",
      "         0.0523,  0.0700, -0.0022, -0.0425, -0.0134, -0.0769, -0.0520,  0.0407,\n",
      "        -0.0046,  0.0687,  0.0868,  0.0993,  0.0983, -0.0805,  0.0266, -0.0181,\n",
      "         0.0901, -0.0626,  0.0842,  0.0357, -0.0784, -0.0172,  0.0207, -0.0355,\n",
      "        -0.0046,  0.0343], dtype=torch.float32, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0616, -0.1268,  0.0117,  ...,  0.0791, -0.1039,  0.0620],\n",
      "        [-0.1223,  0.0955, -0.1024,  ...,  0.0079, -0.0387, -0.0293],\n",
      "        [-0.1380, -0.1351, -0.0867,  ..., -0.0622,  0.0459, -0.0411],\n",
      "        ...,\n",
      "        [ 0.0642, -0.0564,  0.0228,  ...,  0.1120, -0.1157,  0.1022],\n",
      "        [ 0.0499,  0.1038, -0.1036,  ...,  0.1240, -0.0313,  0.0193],\n",
      "        [-0.1137,  0.1173, -0.1016,  ...,  0.0202,  0.1099,  0.1058]],\n",
      "       dtype=torch.float32, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0159, -0.0614, -0.0953,  0.0398,  0.1060,  0.0883, -0.1247, -0.0555,\n",
      "        -0.0795, -0.0124, -0.1384, -0.1200,  0.0904, -0.0292, -0.0154, -0.0516,\n",
      "        -0.0360, -0.0381,  0.0178, -0.0247, -0.0504,  0.1342, -0.1413, -0.1268,\n",
      "        -0.1208], dtype=torch.float32, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for each in theta:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once gradients have been computed using loss.backward()\n",
    "loss.backward()\n",
    "#Calling optimizer.step() updates the parameters according to the optimization algorithm.\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.9925305247306824\n",
      "epoch:  1  loss:  0.9880533218383789\n",
      "epoch:  2  loss:  0.9837154150009155\n",
      "epoch:  3  loss:  0.9795017838478088\n",
      "epoch:  4  loss:  0.9754605889320374\n",
      "epoch:  5  loss:  0.9716665148735046\n",
      "epoch:  6  loss:  0.9680467247962952\n",
      "epoch:  7  loss:  0.9645963311195374\n",
      "epoch:  8  loss:  0.9613767862319946\n",
      "epoch:  9  loss:  0.9582943916320801\n",
      "epoch:  10  loss:  0.9553154706954956\n",
      "epoch:  11  loss:  0.9525396227836609\n",
      "epoch:  12  loss:  0.9499474763870239\n",
      "epoch:  13  loss:  0.9474191069602966\n",
      "epoch:  14  loss:  0.9449619650840759\n",
      "epoch:  15  loss:  0.942597508430481\n",
      "epoch:  16  loss:  0.9403333067893982\n",
      "epoch:  17  loss:  0.9381005167961121\n",
      "epoch:  18  loss:  0.9359002113342285\n",
      "epoch:  19  loss:  0.9337812662124634\n",
      "epoch:  20  loss:  0.9316959381103516\n",
      "epoch:  21  loss:  0.9296768307685852\n",
      "epoch:  22  loss:  0.9277480244636536\n",
      "epoch:  23  loss:  0.9258564114570618\n",
      "epoch:  24  loss:  0.9239720106124878\n",
      "epoch:  25  loss:  0.9221180081367493\n",
      "epoch:  26  loss:  0.9202748537063599\n",
      "epoch:  27  loss:  0.9184477925300598\n",
      "epoch:  28  loss:  0.9166426658630371\n",
      "epoch:  29  loss:  0.9148735404014587\n",
      "epoch:  30  loss:  0.9131534695625305\n",
      "epoch:  31  loss:  0.9114537835121155\n",
      "epoch:  32  loss:  0.9097176790237427\n",
      "epoch:  33  loss:  0.908014178276062\n",
      "epoch:  34  loss:  0.9062425494194031\n",
      "epoch:  35  loss:  0.904515266418457\n",
      "epoch:  36  loss:  0.9027856588363647\n",
      "epoch:  37  loss:  0.9011247754096985\n",
      "epoch:  38  loss:  0.8994355201721191\n",
      "epoch:  39  loss:  0.897760272026062\n",
      "epoch:  40  loss:  0.8960960507392883\n",
      "epoch:  41  loss:  0.894439697265625\n",
      "epoch:  42  loss:  0.892798662185669\n",
      "epoch:  43  loss:  0.8911644220352173\n",
      "epoch:  44  loss:  0.8895253539085388\n",
      "epoch:  45  loss:  0.8879205584526062\n",
      "epoch:  46  loss:  0.8863233923912048\n",
      "epoch:  47  loss:  0.8847454190254211\n",
      "epoch:  48  loss:  0.8831411004066467\n",
      "epoch:  49  loss:  0.881571888923645\n"
     ]
    }
   ],
   "source": [
    "# For Step by Step we can print the iterations\n",
    "for epoch in range(50):\n",
    "   # Forward pass: Compute predicted y by passing x to the model\n",
    "   y_pred = model(x)\n",
    "\n",
    "   # Compute and print loss\n",
    "   loss = criterion(y_pred, target)\n",
    "   print('epoch: ', epoch,' loss: ', loss.item())#loss.item gives us the real number instead of the tensor value.\n",
    "\n",
    "   # Zero gradients, perform a backward pass, and update the weights.\n",
    "   optimizer.zero_grad()\n",
    "\n",
    "   # perform a backward pass (backpropagation)\n",
    "   loss.backward()\n",
    "\n",
    "   # Update the parameters\n",
    "   optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After above iterations we see that the weights have still pretty large loss...\n",
    "# this is because we have to either chose a little larger learning rate or else we have iterate moder no. of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us update the Learning Rate to 1 and check\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.8800033926963806\n",
      "epoch:  1  loss:  0.8667050004005432\n",
      "epoch:  2  loss:  0.8524036407470703\n",
      "epoch:  3  loss:  0.8395223617553711\n",
      "epoch:  4  loss:  0.8281170129776001\n",
      "epoch:  5  loss:  0.816463053226471\n",
      "epoch:  6  loss:  0.8059546947479248\n",
      "epoch:  7  loss:  0.7958323955535889\n",
      "epoch:  8  loss:  0.7862088680267334\n",
      "epoch:  9  loss:  0.7763833999633789\n",
      "epoch:  10  loss:  0.7675084471702576\n",
      "epoch:  11  loss:  0.7592566013336182\n",
      "epoch:  12  loss:  0.7522581219673157\n",
      "epoch:  13  loss:  0.7446271181106567\n",
      "epoch:  14  loss:  0.7375498414039612\n",
      "epoch:  15  loss:  0.731005847454071\n",
      "epoch:  16  loss:  0.7258388996124268\n",
      "epoch:  17  loss:  0.7205127477645874\n",
      "epoch:  18  loss:  0.7148871421813965\n",
      "epoch:  19  loss:  0.7096412181854248\n",
      "epoch:  20  loss:  0.7046909332275391\n",
      "epoch:  21  loss:  0.7004160284996033\n",
      "epoch:  22  loss:  0.6962743997573853\n",
      "epoch:  23  loss:  0.6926296353340149\n",
      "epoch:  24  loss:  0.689002275466919\n",
      "epoch:  25  loss:  0.685852587223053\n",
      "epoch:  26  loss:  0.6825329065322876\n",
      "epoch:  27  loss:  0.6798794269561768\n",
      "epoch:  28  loss:  0.6775327324867249\n",
      "epoch:  29  loss:  0.6748916506767273\n",
      "epoch:  30  loss:  0.6724982857704163\n",
      "epoch:  31  loss:  0.670555055141449\n",
      "epoch:  32  loss:  0.6685782670974731\n",
      "epoch:  33  loss:  0.6669004559516907\n",
      "epoch:  34  loss:  0.6653294563293457\n",
      "epoch:  35  loss:  0.6636696457862854\n",
      "epoch:  36  loss:  0.6623721122741699\n",
      "epoch:  37  loss:  0.6606908440589905\n",
      "epoch:  38  loss:  0.6593182682991028\n",
      "epoch:  39  loss:  0.6582379341125488\n",
      "epoch:  40  loss:  0.6570029854774475\n",
      "epoch:  41  loss:  0.6558837294578552\n",
      "epoch:  42  loss:  0.6549450159072876\n",
      "epoch:  43  loss:  0.6540946960449219\n",
      "epoch:  44  loss:  0.6530361771583557\n",
      "epoch:  45  loss:  0.6523242592811584\n",
      "epoch:  46  loss:  0.6516140699386597\n",
      "epoch:  47  loss:  0.6510182023048401\n",
      "epoch:  48  loss:  0.6504946351051331\n",
      "epoch:  49  loss:  0.6498849391937256\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "   # Forward pass: Compute predicted y by passing x to the model\n",
    "   y_pred = model(x)\n",
    "\n",
    "   # Compute and print loss\n",
    "   loss = criterion(y_pred, target)\n",
    "   print('epoch: ', epoch,' loss: ', loss.item())#loss.item gives us the real number instead of the tensor value.\n",
    "\n",
    "   # Zero gradients, perform a backward pass, and update the weights.\n",
    "   optimizer.zero_grad()\n",
    "\n",
    "   # perform a backward pass (backpropagation)\n",
    "   loss.backward()\n",
    "\n",
    "   # Update the parameters\n",
    "   optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.6091945767402649\n",
      "epoch:  1  loss:  0.6091945171356201\n",
      "epoch:  2  loss:  0.6091945171356201\n",
      "epoch:  3  loss:  0.6091944575309753\n",
      "epoch:  4  loss:  0.6091943979263306\n",
      "epoch:  5  loss:  0.6091943383216858\n",
      "epoch:  6  loss:  0.6091943383216858\n",
      "epoch:  7  loss:  0.609194278717041\n",
      "epoch:  8  loss:  0.6091942191123962\n",
      "epoch:  9  loss:  0.6091942191123962\n",
      "epoch:  10  loss:  0.6091941595077515\n",
      "epoch:  11  loss:  0.6091940999031067\n",
      "epoch:  12  loss:  0.6091940999031067\n",
      "epoch:  13  loss:  0.6091940402984619\n",
      "epoch:  14  loss:  0.6091940402984619\n",
      "epoch:  15  loss:  0.6091939806938171\n",
      "epoch:  16  loss:  0.6091939806938171\n",
      "epoch:  17  loss:  0.6091939210891724\n",
      "epoch:  18  loss:  0.6091939210891724\n",
      "epoch:  19  loss:  0.6091938614845276\n",
      "epoch:  20  loss:  0.6091938614845276\n",
      "epoch:  21  loss:  0.6091938018798828\n",
      "epoch:  22  loss:  0.6091938018798828\n",
      "epoch:  23  loss:  0.6091938018798828\n",
      "epoch:  24  loss:  0.609193742275238\n",
      "epoch:  25  loss:  0.609193742275238\n",
      "epoch:  26  loss:  0.6091936826705933\n",
      "epoch:  27  loss:  0.6091936826705933\n",
      "epoch:  28  loss:  0.6091936826705933\n",
      "epoch:  29  loss:  0.6091936230659485\n",
      "epoch:  30  loss:  0.6091936230659485\n",
      "epoch:  31  loss:  0.6091936230659485\n",
      "epoch:  32  loss:  0.6091935634613037\n",
      "epoch:  33  loss:  0.6091935634613037\n",
      "epoch:  34  loss:  0.6091935634613037\n",
      "epoch:  35  loss:  0.6091935038566589\n",
      "epoch:  36  loss:  0.6091935038566589\n",
      "epoch:  37  loss:  0.6091935038566589\n",
      "epoch:  38  loss:  0.6091935038566589\n",
      "epoch:  39  loss:  0.6091934442520142\n",
      "epoch:  40  loss:  0.6091934442520142\n",
      "epoch:  41  loss:  0.6091934442520142\n",
      "epoch:  42  loss:  0.6091934442520142\n",
      "epoch:  43  loss:  0.6091933846473694\n",
      "epoch:  44  loss:  0.6091933846473694\n",
      "epoch:  45  loss:  0.6091933846473694\n",
      "epoch:  46  loss:  0.6091933846473694\n",
      "epoch:  47  loss:  0.6091933846473694\n",
      "epoch:  48  loss:  0.6091933250427246\n",
      "epoch:  49  loss:  0.6091933250427246\n",
      "epoch:  50  loss:  0.6091933250427246\n",
      "epoch:  51  loss:  0.6091933250427246\n",
      "epoch:  52  loss:  0.6091933250427246\n",
      "epoch:  53  loss:  0.6091932654380798\n",
      "epoch:  54  loss:  0.6091932654380798\n",
      "epoch:  55  loss:  0.6091932654380798\n",
      "epoch:  56  loss:  0.6091932654380798\n",
      "epoch:  57  loss:  0.6091932654380798\n",
      "epoch:  58  loss:  0.6091932654380798\n",
      "epoch:  59  loss:  0.6091932654380798\n",
      "epoch:  60  loss:  0.6091932058334351\n",
      "epoch:  61  loss:  0.6091932058334351\n",
      "epoch:  62  loss:  0.6091932058334351\n",
      "epoch:  63  loss:  0.6091932058334351\n",
      "epoch:  64  loss:  0.6091932058334351\n",
      "epoch:  65  loss:  0.6091932058334351\n",
      "epoch:  66  loss:  0.6091932058334351\n",
      "epoch:  67  loss:  0.6091932058334351\n",
      "epoch:  68  loss:  0.6091931462287903\n",
      "epoch:  69  loss:  0.6091931462287903\n",
      "epoch:  70  loss:  0.6091931462287903\n",
      "epoch:  71  loss:  0.6091931462287903\n",
      "epoch:  72  loss:  0.6091931462287903\n",
      "epoch:  73  loss:  0.6091931462287903\n",
      "epoch:  74  loss:  0.6091931462287903\n",
      "epoch:  75  loss:  0.6091931462287903\n",
      "epoch:  76  loss:  0.6091931462287903\n",
      "epoch:  77  loss:  0.6091931462287903\n",
      "epoch:  78  loss:  0.6091931462287903\n",
      "epoch:  79  loss:  0.6091930866241455\n",
      "epoch:  80  loss:  0.6091930866241455\n",
      "epoch:  81  loss:  0.6091930866241455\n",
      "epoch:  82  loss:  0.6091930866241455\n",
      "epoch:  83  loss:  0.6091930866241455\n",
      "epoch:  84  loss:  0.6091930866241455\n",
      "epoch:  85  loss:  0.6091930866241455\n",
      "epoch:  86  loss:  0.6091930866241455\n",
      "epoch:  87  loss:  0.6091930866241455\n",
      "epoch:  88  loss:  0.6091930866241455\n",
      "epoch:  89  loss:  0.6091930866241455\n",
      "epoch:  90  loss:  0.6091930866241455\n",
      "epoch:  91  loss:  0.6091930866241455\n",
      "epoch:  92  loss:  0.6091930866241455\n",
      "epoch:  93  loss:  0.6091930866241455\n",
      "epoch:  94  loss:  0.6091930270195007\n",
      "epoch:  95  loss:  0.6091930270195007\n",
      "epoch:  96  loss:  0.6091930270195007\n",
      "epoch:  97  loss:  0.6091930270195007\n",
      "epoch:  98  loss:  0.6091930270195007\n",
      "epoch:  99  loss:  0.6091930270195007\n",
      "epoch:  100  loss:  0.6091930270195007\n",
      "epoch:  101  loss:  0.6091930270195007\n",
      "epoch:  102  loss:  0.6091930270195007\n",
      "epoch:  103  loss:  0.6091930270195007\n",
      "epoch:  104  loss:  0.6091930270195007\n",
      "epoch:  105  loss:  0.6091930270195007\n",
      "epoch:  106  loss:  0.6091930270195007\n",
      "epoch:  107  loss:  0.6091930270195007\n",
      "epoch:  108  loss:  0.6091930270195007\n",
      "epoch:  109  loss:  0.6091930270195007\n",
      "epoch:  110  loss:  0.6091930270195007\n",
      "epoch:  111  loss:  0.6091930270195007\n",
      "epoch:  112  loss:  0.6091930270195007\n",
      "epoch:  113  loss:  0.6091930270195007\n",
      "epoch:  114  loss:  0.6091930270195007\n",
      "epoch:  115  loss:  0.6091930270195007\n",
      "epoch:  116  loss:  0.6091930270195007\n",
      "epoch:  117  loss:  0.6091930270195007\n",
      "epoch:  118  loss:  0.6091930270195007\n",
      "epoch:  119  loss:  0.6091930270195007\n",
      "epoch:  120  loss:  0.6091930270195007\n",
      "epoch:  121  loss:  0.6091930270195007\n",
      "epoch:  122  loss:  0.6091930270195007\n",
      "epoch:  123  loss:  0.6091930270195007\n",
      "epoch:  124  loss:  0.609192967414856\n",
      "epoch:  125  loss:  0.609192967414856\n",
      "epoch:  126  loss:  0.609192967414856\n",
      "epoch:  127  loss:  0.609192967414856\n",
      "epoch:  128  loss:  0.609192967414856\n",
      "epoch:  129  loss:  0.609192967414856\n",
      "epoch:  130  loss:  0.609192967414856\n",
      "epoch:  131  loss:  0.609192967414856\n",
      "epoch:  132  loss:  0.609192967414856\n",
      "epoch:  133  loss:  0.609192967414856\n",
      "epoch:  134  loss:  0.609192967414856\n",
      "epoch:  135  loss:  0.609192967414856\n",
      "epoch:  136  loss:  0.609192967414856\n",
      "epoch:  137  loss:  0.609192967414856\n",
      "epoch:  138  loss:  0.609192967414856\n",
      "epoch:  139  loss:  0.609192967414856\n",
      "epoch:  140  loss:  0.609192967414856\n",
      "epoch:  141  loss:  0.609192967414856\n",
      "epoch:  142  loss:  0.609192967414856\n",
      "epoch:  143  loss:  0.609192967414856\n",
      "epoch:  144  loss:  0.609192967414856\n",
      "epoch:  145  loss:  0.609192967414856\n",
      "epoch:  146  loss:  0.609192967414856\n",
      "epoch:  147  loss:  0.609192967414856\n",
      "epoch:  148  loss:  0.609192967414856\n",
      "epoch:  149  loss:  0.609192967414856\n",
      "epoch:  150  loss:  0.609192967414856\n",
      "epoch:  151  loss:  0.609192967414856\n",
      "epoch:  152  loss:  0.609192967414856\n",
      "epoch:  153  loss:  0.609192967414856\n",
      "epoch:  154  loss:  0.609192967414856\n",
      "epoch:  155  loss:  0.609192967414856\n",
      "epoch:  156  loss:  0.609192967414856\n",
      "epoch:  157  loss:  0.609192967414856\n",
      "epoch:  158  loss:  0.609192967414856\n",
      "epoch:  159  loss:  0.609192967414856\n",
      "epoch:  160  loss:  0.609192967414856\n",
      "epoch:  161  loss:  0.609192967414856\n",
      "epoch:  162  loss:  0.609192967414856\n",
      "epoch:  163  loss:  0.609192967414856\n",
      "epoch:  164  loss:  0.609192967414856\n",
      "epoch:  165  loss:  0.609192967414856\n",
      "epoch:  166  loss:  0.609192967414856\n",
      "epoch:  167  loss:  0.609192967414856\n",
      "epoch:  168  loss:  0.609192967414856\n",
      "epoch:  169  loss:  0.609192967414856\n",
      "epoch:  170  loss:  0.609192967414856\n",
      "epoch:  171  loss:  0.609192967414856\n",
      "epoch:  172  loss:  0.609192967414856\n",
      "epoch:  173  loss:  0.609192967414856\n",
      "epoch:  174  loss:  0.609192967414856\n",
      "epoch:  175  loss:  0.609192967414856\n",
      "epoch:  176  loss:  0.609192967414856\n",
      "epoch:  177  loss:  0.609192967414856\n",
      "epoch:  178  loss:  0.609192967414856\n",
      "epoch:  179  loss:  0.609192967414856\n",
      "epoch:  180  loss:  0.609192967414856\n",
      "epoch:  181  loss:  0.609192967414856\n",
      "epoch:  182  loss:  0.609192967414856\n",
      "epoch:  183  loss:  0.609192967414856\n",
      "epoch:  184  loss:  0.609192967414856\n",
      "epoch:  185  loss:  0.609192967414856\n",
      "epoch:  186  loss:  0.609192967414856\n",
      "epoch:  187  loss:  0.609192967414856\n",
      "epoch:  188  loss:  0.609192967414856\n",
      "epoch:  189  loss:  0.609192967414856\n",
      "epoch:  190  loss:  0.609192967414856\n",
      "epoch:  191  loss:  0.609192967414856\n",
      "epoch:  192  loss:  0.609192967414856\n",
      "epoch:  193  loss:  0.609192967414856\n",
      "epoch:  194  loss:  0.609192967414856\n",
      "epoch:  195  loss:  0.609192967414856\n",
      "epoch:  196  loss:  0.609192967414856\n",
      "epoch:  197  loss:  0.609192967414856\n",
      "epoch:  198  loss:  0.609192967414856\n",
      "epoch:  199  loss:  0.609192967414856\n",
      "epoch:  200  loss:  0.609192967414856\n",
      "epoch:  201  loss:  0.609192967414856\n",
      "epoch:  202  loss:  0.609192967414856\n",
      "epoch:  203  loss:  0.609192967414856\n",
      "epoch:  204  loss:  0.609192967414856\n",
      "epoch:  205  loss:  0.609192967414856\n",
      "epoch:  206  loss:  0.609192967414856\n",
      "epoch:  207  loss:  0.609192967414856\n",
      "epoch:  208  loss:  0.609192967414856\n",
      "epoch:  209  loss:  0.609192967414856\n",
      "epoch:  210  loss:  0.609192967414856\n",
      "epoch:  211  loss:  0.609192967414856\n",
      "epoch:  212  loss:  0.609192967414856\n",
      "epoch:  213  loss:  0.609192967414856\n",
      "epoch:  214  loss:  0.609192967414856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  215  loss:  0.609192967414856\n",
      "epoch:  216  loss:  0.609192967414856\n",
      "epoch:  217  loss:  0.609192967414856\n",
      "epoch:  218  loss:  0.609192967414856\n",
      "epoch:  219  loss:  0.609192967414856\n",
      "epoch:  220  loss:  0.609192967414856\n",
      "epoch:  221  loss:  0.609192967414856\n",
      "epoch:  222  loss:  0.609192967414856\n",
      "epoch:  223  loss:  0.609192967414856\n",
      "epoch:  224  loss:  0.609192967414856\n",
      "epoch:  225  loss:  0.609192967414856\n",
      "epoch:  226  loss:  0.609192967414856\n",
      "epoch:  227  loss:  0.609192967414856\n",
      "epoch:  228  loss:  0.609192967414856\n",
      "epoch:  229  loss:  0.609192967414856\n",
      "epoch:  230  loss:  0.609192967414856\n",
      "epoch:  231  loss:  0.609192967414856\n",
      "epoch:  232  loss:  0.609192967414856\n",
      "epoch:  233  loss:  0.609192967414856\n",
      "epoch:  234  loss:  0.609192967414856\n",
      "epoch:  235  loss:  0.609192967414856\n",
      "epoch:  236  loss:  0.609192967414856\n",
      "epoch:  237  loss:  0.609192967414856\n",
      "epoch:  238  loss:  0.609192967414856\n",
      "epoch:  239  loss:  0.609192967414856\n",
      "epoch:  240  loss:  0.609192967414856\n",
      "epoch:  241  loss:  0.609192967414856\n",
      "epoch:  242  loss:  0.609192967414856\n",
      "epoch:  243  loss:  0.609192967414856\n",
      "epoch:  244  loss:  0.609192967414856\n",
      "epoch:  245  loss:  0.609192967414856\n",
      "epoch:  246  loss:  0.609192967414856\n",
      "epoch:  247  loss:  0.609192967414856\n",
      "epoch:  248  loss:  0.609192967414856\n",
      "epoch:  249  loss:  0.609192967414856\n",
      "epoch:  250  loss:  0.609192967414856\n",
      "epoch:  251  loss:  0.609192967414856\n",
      "epoch:  252  loss:  0.609192967414856\n",
      "epoch:  253  loss:  0.609192967414856\n",
      "epoch:  254  loss:  0.609192967414856\n",
      "epoch:  255  loss:  0.609192967414856\n",
      "epoch:  256  loss:  0.609192967414856\n",
      "epoch:  257  loss:  0.609192967414856\n",
      "epoch:  258  loss:  0.609192967414856\n",
      "epoch:  259  loss:  0.609192967414856\n",
      "epoch:  260  loss:  0.609192967414856\n",
      "epoch:  261  loss:  0.609192967414856\n",
      "epoch:  262  loss:  0.609192967414856\n",
      "epoch:  263  loss:  0.609192967414856\n",
      "epoch:  264  loss:  0.609192967414856\n",
      "epoch:  265  loss:  0.609192967414856\n",
      "epoch:  266  loss:  0.609192967414856\n",
      "epoch:  267  loss:  0.609192967414856\n",
      "epoch:  268  loss:  0.609192967414856\n",
      "epoch:  269  loss:  0.609192967414856\n",
      "epoch:  270  loss:  0.609192967414856\n",
      "epoch:  271  loss:  0.609192967414856\n",
      "epoch:  272  loss:  0.609192967414856\n",
      "epoch:  273  loss:  0.609192967414856\n",
      "epoch:  274  loss:  0.609192967414856\n",
      "epoch:  275  loss:  0.609192967414856\n",
      "epoch:  276  loss:  0.609192967414856\n",
      "epoch:  277  loss:  0.609192967414856\n",
      "epoch:  278  loss:  0.609192967414856\n",
      "epoch:  279  loss:  0.609192967414856\n",
      "epoch:  280  loss:  0.609192967414856\n",
      "epoch:  281  loss:  0.609192967414856\n",
      "epoch:  282  loss:  0.609192967414856\n",
      "epoch:  283  loss:  0.609192967414856\n",
      "epoch:  284  loss:  0.609192967414856\n",
      "epoch:  285  loss:  0.609192967414856\n",
      "epoch:  286  loss:  0.609192967414856\n",
      "epoch:  287  loss:  0.609192967414856\n",
      "epoch:  288  loss:  0.609192967414856\n",
      "epoch:  289  loss:  0.609192967414856\n",
      "epoch:  290  loss:  0.609192967414856\n",
      "epoch:  291  loss:  0.609192967414856\n",
      "epoch:  292  loss:  0.609192967414856\n",
      "epoch:  293  loss:  0.609192967414856\n",
      "epoch:  294  loss:  0.609192967414856\n",
      "epoch:  295  loss:  0.609192967414856\n",
      "epoch:  296  loss:  0.609192967414856\n",
      "epoch:  297  loss:  0.609192967414856\n",
      "epoch:  298  loss:  0.609192967414856\n",
      "epoch:  299  loss:  0.609192967414856\n"
     ]
    }
   ],
   "source": [
    "# Still it has not reached the minima \n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=6)\n",
    "# So now we again change the LR and increse the no. of iterations\n",
    "for epoch in range(300):\n",
    "   # Forward pass: Compute predicted y by passing x to the model\n",
    "   y_pred = model(x)\n",
    "\n",
    "   # Compute and print loss\n",
    "   loss = criterion(y_pred, target)\n",
    "   print('epoch: ', epoch,' loss: ', loss.item())#loss.item gives us the real number instead of the tensor value.\n",
    "\n",
    "   # Zero gradients, perform a backward pass, and update the weights.\n",
    "   optimizer.zero_grad()\n",
    "\n",
    "   # perform a backward pass (backpropagation)\n",
    "   loss.backward()\n",
    "\n",
    "   # Update the parameters\n",
    "   optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since Here even after several no. of iterations we were not able to decrease the loss this is because of the dummy data we have chosen which is totally inapproriate and therei is no meaningful relationship between the input data and the target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
